---
Id: 1028
Title: "Day 28: Robust Imputation and Numeric Coercion"
Author: Sughosh P Dixit
Date: "2025-11-28"
Tags: Data Science Imputation Missing Values Numeric Coercion Data Quality Preprocessing Statistics
Topic: Data Science
Abstract: "Understand how numeric coercion and NA handling affect data distributions. Learn the impact of different imputation strategies on mean, variance, and quantiles for threshold-based rule evaluation."
HeaderImage: /DS-28/imputation_intro.png
isPublished: false
---

# **Day 28: Robust Imputation and Numeric Coercion** ğŸ”§ğŸ“Š

<p style={{fontStyle: 'italic', color: '#666', marginTop: '1rem', textAlign: 'center'}}>Understand how data preprocessing choices affect your distributions and downstream thresholds.</p>

<Lottie animation="analyticsPulse" height={240} width={340} caption="Imputation and coercion choices directly impact statistical measures and threshold calibrationâ€”choose wisely." />

<p style={{fontStyle: 'italic', color: '#666', margin: '1rem 0 2rem', textAlign: 'center'}}>Before rules can be evaluated, data must be clean. How you handle missing values and convert data types has profound effects on the distributions your thresholds are based on.</p>

> ğŸ’¡ **Note:** This article uses technical terms and abbreviations. For definitions, check out the [Key Terms & Glossary](/key) page.

---

## The Problem: Missing Data and Type Mismatches ğŸ¯

**Scenario:** Your fraud detection pipeline receives raw data with:
- Missing values (NA, NULL, "")
- Mixed types ("100", 100, "N/A")
- Invalid entries ("error", -999)

**Questions:**
1. How do you convert everything to numeric?
2. What do you replace missing values with?
3. How do these choices affect thresholds?

**The answer matters more than you think!** ğŸ¤”

---

## Numeric Coercion ğŸ”¢

### What is Coercion?

**Coercion** is converting data from one type to another.

**Common scenarios:**
- String â†’ Number: "100" â†’ 100
- Number â†’ Integer: 100.7 â†’ 100 or 101
- Invalid â†’ NA: "error" â†’ NA

### Coercion in Practice

```python
def safe_numeric(value, default=None):
    """
    Safely convert to numeric with fallback.
    """
    try:
        return float(value)
    except (ValueError, TypeError):
        return default
```

### Edge Cases

**What happens with:**
```python
safe_numeric("100")      # â†’ 100.0
safe_numeric("$100")     # â†’ None (or clean first)
safe_numeric("")         # â†’ None
safe_numeric(None)       # â†’ None
safe_numeric("1e10")     # â†’ 10000000000.0
safe_numeric("NaN")      # â†’ nan (careful!)
```

**Visual Example:**

<img src="/DS-28/coercion_examples.png" alt="Coercion Examples" style={{maxWidth: '100%', height: 'auto', display: 'block', margin: '1.5rem auto', borderRadius: '8px'}} />

<Lottie animation="robustWorkflow" height={220} width={320} caption="Coercion must handle edge cases gracefully to prevent downstream errors in rule evaluation." />

---

## Imputation Strategies ğŸ©¹

### Strategy 1: Impute with Zero

**Method:** Replace NA with 0

```python
df['amount'] = df['amount'].fillna(0)
```

**When appropriate:**
- NA genuinely means "no activity"
- Zero is a valid value in the domain
- You want to flag non-activity

**Risks:**
- Shifts distribution left
- Inflates zero-count
- Can dramatically change quantiles

### Strategy 2: Impute with Mean

**Method:** Replace NA with the column mean

```python
df['amount'] = df['amount'].fillna(df['amount'].mean())
```

**When appropriate:**
- Missing at random (MAR)
- Want to preserve mean
- Large sample sizes

**Risks:**
- Reduces variance
- Can distort relationships
- Ignores missingness pattern

### Strategy 3: Impute with Median

**Method:** Replace NA with the column median

```python
df['amount'] = df['amount'].fillna(df['amount'].median())
```

**When appropriate:**
- Skewed distributions
- Outliers present
- Want robust central tendency

**Risks:**
- Still reduces variance
- Ignores missingness pattern
- May create spike at median

### Strategy 4: Keep as NA (Exclude)

**Method:** Leave missing and handle separately

```python
df_clean = df.dropna(subset=['amount'])
```

**When appropriate:**
- Missingness is informative
- Separate rules for missing
- Small fraction missing

**Risks:**
- Reduces sample size
- Selection bias if not MCAR

**Visual Example:**

<img src="/DS-28/imputation_strategies.png" alt="Imputation Strategies" style={{maxWidth: '100%', height: 'auto', display: 'block', margin: '1.5rem auto', borderRadius: '8px'}} />

---

## Impact on Distribution Statistics ğŸ“ˆ

### Mean

**Original:** Î¼ = Î£x_i / n

**After zero imputation:**
- Mean decreases (zeros pull it down)
- Effect proportional to missing rate

**After mean imputation:**
- Mean preserved (by design)
- But artificial values added

### Variance

**Original:** ÏƒÂ² = Î£(x_i - Î¼)Â² / n

**After zero imputation:**
- Variance increases if mean > 0
- Creates bimodal distribution

**After mean imputation:**
- Variance decreases
- Imputed values have zero deviation

**Formula for variance reduction:**
```
ÏƒÂ²_new â‰ˆ ÏƒÂ²_old Ã— (1 - missing_rate)
```

### Quantiles

**Zero imputation effect:**
```
Original: [10, 20, 30, 40, 50]  â†’ p50 = 30
With 20% zeros: [0, 10, 20, 30, 40, 50]  â†’ p50 = 25 (shifted!)
```

**Median imputation effect:**
```
Original: [10, 20, 30, 40, 50]  â†’ p50 = 30
With imputed 30s: [10, 20, 30, 30, 30, 40, 50]  â†’ p50 = 30 (preserved)
But p75 = 30 (distorted!)
```

**Visual Example:**

<img src="/DS-28/distribution_impact.png" alt="Distribution Impact" style={{maxWidth: '100%', height: 'auto', display: 'block', margin: '1.5rem auto', borderRadius: '8px'}} />

<Lottie animation="breakdownFortress" height={220} width={320} caption="Different imputation methods shift distributions in different waysâ€”understand the trade-offs." />

---

## Rule Geometry Changes ğŸ“

### How fillna(0) Changes Rules

**Before imputation:**
```
Rule: IF amount > threshold THEN flag

Data: [10, 20, NA, 40, 50]
Threshold (p50 excluding NA): 30
Flagged: [40, 50] (2 values)
```

**After fillna(0):**
```
Data: [10, 20, 0, 40, 50]
Threshold (p50): 20  â† Lower!
Flagged: [40, 50] (still 2, but threshold changed)
```

### Geometric Interpretation

**In feature space:**
- Zero imputation adds points at the origin
- This shifts decision boundaries
- Thresholds based on quantiles move

**Example: 2D space**
```
Before: Points scattered in positive quadrant
After: Cluster of zeros at (0, 0)

Decision boundary must now separate:
- True zeros (valid)
- Imputed zeros (missing)
- Positive values
```

**Visual Example:**

<img src="/DS-28/rule_geometry.png" alt="Rule Geometry Changes" style={{maxWidth: '100%', height: 'auto', display: 'block', margin: '1.5rem auto', borderRadius: '8px'}} />

---

## Histograms: Before and After ğŸ“Š

### Visual Comparison

**Original distribution (no missing):**
```
      â”‚    â–„â–„
      â”‚   â–ˆâ–ˆâ–ˆâ–ˆ
      â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
      â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
      â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       0   20   40   60
```

**After zero imputation (20% missing):**
```
      â”‚â–ˆ
      â”‚â–ˆ   â–„â–„
      â”‚â–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ
      â”‚â–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
      â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       0   20   40   60
       â†‘
      Spike!
```

**After mean imputation:**
```
      â”‚    â–ˆâ–„
      â”‚   â–ˆâ–ˆâ–ˆâ–ˆ
      â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
      â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
      â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       0   20   40   60
           â†‘
         Spike at mean
```

**Visual Example:**

<img src="/DS-28/histograms_comparison.png" alt="Histograms Before/After" style={{maxWidth: '100%', height: 'auto', display: 'block', margin: '1.5rem auto', borderRadius: '8px'}} />

---

## Exercise: Quantifying Percentile Shift ğŸ“

### The Problem

**Given:** A skewed sample with 20% missing values.

**Original data (no missing):**
```
[5, 10, 15, 20, 25, 30, 50, 100, 150, 200]
```

**Task:** Quantify how replacing 2 random values with NA, then imputing with 0, shifts the 95th percentile.

### Solution

**Step 1: Original 95th Percentile**

```python
data = [5, 10, 15, 20, 25, 30, 50, 100, 150, 200]
p95_original = np.percentile(data, 95)
# p95_original â‰ˆ 187.5 (interpolated)
```

**Step 2: Remove 2 Values (20% missing)**

Suppose we remove indices 3 and 7 (values 20 and 100):
```
data_with_na = [5, 10, 15, NA, 25, 30, 50, NA, 150, 200]
```

**Step 3: Impute with Zero**

```
data_imputed = [5, 10, 15, 0, 25, 30, 50, 0, 150, 200]
Sorted: [0, 0, 5, 10, 15, 25, 30, 50, 150, 200]
```

**Step 4: New 95th Percentile**

```python
p95_imputed = np.percentile(data_imputed, 95)
# p95_imputed â‰ˆ 187.5 (high percentile less affected)
```

**Step 5: Effect on Lower Percentiles**

```python
p50_original = np.percentile(data, 50)  # â‰ˆ 27.5
p50_imputed = np.percentile(data_imputed, 50)  # â‰ˆ 20

Shift: 27.5 - 20 = 7.5 (27% reduction!)
```

### Key Insights

1. **High percentiles (90th, 95th):** Less affected by zero imputation
2. **Median (50th):** Significantly shifted down
3. **Lower percentiles (10th, 25th):** Pushed to zero
4. **Effect is proportional to:** Missing rate and zero distance from median

**Visual Example:**

<img src="/DS-28/exercise_percentile.png" alt="Exercise Percentile Shift" style={{maxWidth: '100%', height: 'auto', display: 'block', margin: '1.5rem auto', borderRadius: '8px'}} />

<Lottie animation="classicalVsRobust" height={220} width={320} caption="Zero imputation primarily affects lower and middle percentiles, with diminishing effect on extreme upper percentiles." />

---

## Best Practices for Imputation âœ…

### 1. Understand Your Missing Data

```python
# Analyze missing pattern
missing_rate = df.isna().mean()
missing_correlation = df.isna().corr()
```

### 2. Document Your Strategy

```python
IMPUTATION_CONFIG = {
    'amount': {'method': 'zero', 'reason': 'NA means no transaction'},
    'score': {'method': 'median', 'reason': 'Preserve central tendency'},
    'count': {'method': 'exclude', 'reason': 'Analyze separately'},
}
```

### 3. Compare Before and After

```python
def imputation_impact_report(original, imputed, percentiles=[25, 50, 75, 90, 95]):
    report = {}
    for p in percentiles:
        orig = np.percentile(original.dropna(), p)
        imp = np.percentile(imputed, p)
        report[f'p{p}'] = {'original': orig, 'imputed': imp, 'shift': imp - orig}
    return report
```

### 4. Consider Separate Rules for Missing

```python
if pd.isna(value):
    return apply_missing_rule(event)
else:
    return apply_normal_rule(event, value)
```

### 5. Monitor Drift

Track imputation effects over time as missing patterns change.

### 6. Use Robust Statistics

When possible, use median-based methods that resist imputation artifacts.

---

## Summary Table ğŸ“‹

<table>
<thead>
<tr>
<th>Strategy</th>
<th>Effect on Mean</th>
<th>Effect on Variance</th>
<th>Effect on Quantiles</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Zero</strong></td>
<td>â†“ Decreases</td>
<td>â†‘ Increases</td>
<td>â†“ Lower percentiles drop</td>
</tr>
<tr>
<td><strong>Mean</strong></td>
<td>= Preserved</td>
<td>â†“ Decreases</td>
<td>~ Middle compressed</td>
</tr>
<tr>
<td><strong>Median</strong></td>
<td>~ Slight shift</td>
<td>â†“ Decreases</td>
<td>= Median preserved</td>
</tr>
<tr>
<td><strong>Exclude</strong></td>
<td>? Depends</td>
<td>? Depends</td>
<td>? Depends on MCAR</td>
</tr>
</tbody>
</table>

---

## Final Thoughts ğŸŒŸ

Imputation and coercion are not neutral operationsâ€”they actively shape your data distribution:

- **Zero imputation** creates spikes and shifts percentiles down
- **Mean/median imputation** compresses variance
- **Exclusion** may introduce selection bias

**Key Takeaways:**

âœ… **Coercion must handle edge cases** gracefully
âœ… **Zero imputation shifts lower percentiles** significantly
âœ… **Mean imputation reduces variance** artificially
âœ… **Median imputation is more robust** but still affects distribution
âœ… **fillna(0) changes rule geometry** by adding origin points
âœ… **Document and monitor** your imputation choices

**Clean data, clear thresholds!** ğŸ”§ğŸ¯

**Tomorrow's Preview:** Day 29 - Putting It All Together: Constructing a Stratified Audit Plan ğŸ“‹ğŸ¯

---


