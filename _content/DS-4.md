---
Id: 1004
Title: "Day 4 â€” Percentile Rank and Stratifications"
Author: Sughosh P Dixit
Date: "2024-12-04"
Tags: Data Science Statistics Percentile Rank Stratification ECDF Quantiles Sampling
Topic: Data Science
Abstract: "Percentile ranks turn any numeric feature into a simple score in [0,1] that says 'what fraction of the data is at or below this value.' Learn how to combine ranks with min/max and create strata for sampling, prioritization, or analysis."
HeaderImage: /DS-4/percentile_rank_concept.png
isPublished: true
---

# **Day 4 â€” Percentile Rank and Stratification (with Solved Examples)** ğŸ“ˆğŸ¯

<div style={{textAlign: 'center', margin: '2rem 0'}}>
  <div id="lottie-rank" style={{width: '200px', height: '200px', margin: '0 auto'}}></div>
  <p style={{fontStyle: 'italic', color: '#666', marginTop: '1rem'}}>Ranking and stratifying data for insights! ğŸ“Š</p>
</div>

> ğŸ’¡ **Note:** This article uses technical terms and abbreviations. For definitions, check out the [Key Terms & Glossary](/key) page.

---

## ğŸ¯ Introduction

[Percentile ranks](/key) provide a powerful way to normalize features onto a common scale, making it easy to combine multiple features and create meaningful [stratifications](/key) for analysis and sampling.

**TL;DR:**

[Percentile ranks](/key) turn any numeric feature into a simple score in `[0,1]` that says *"what fraction of the data is at or below this value."*

Because percentile ranks are order-based, they are **stable under rescaling** and other [monotone transforms](/key).

If you combine several features' ranks using **min (for AND-like behavior)** or **max (for OR-like behavior)**, you get a single score that you can cut into **[strata](/key)** (e.g., quartiles/deciles) for sampling, prioritization, or analysis. ğŸ’¡

![Percentile Rank Concept](/DS-4/percentile_rank_concept.png)

---

## ğŸ§® Percentile Rank: A Simple [0,1] Scale

* Given a feature **X** and a dataset of size **n**, the [percentile rank](/key) of a value *xáµ¢* is:

  ğŸ“Š `rankáµ¢ = Fâ‚™(xáµ¢) = (1/n) Ã— (# of values â‰¤ xáµ¢)`

* This maps each observation to a number between **0 and 1** (inclusive).

* âœ¨ Properties you get *for free*:

  * ğŸ”¼ **Monotonicity (isotonicity):** If xáµ¢ â‰¤ xâ±¼, then rankáµ¢ â‰¤ rankâ±¼.

  * ğŸ” **Invariance to monotone transforms:** If f is strictly increasing (e.g., aÂ·x+b with a>0), ranks don't change.

ğŸ’¡ *Why this matters:* Different features may have different scales or units. Percentile ranks put everything onto the same comparable [0,1] scale, making multi-feature logic easy to reason about.

![Percentile Rank Properties](/DS-4/rank_properties.png)

---

## âš™ï¸ Combining percentile ranks across features

If you have features **A** and **B** with ranks *rA* and *rB*:

* ğŸ”’ **Conservative (AND-like) combination:**

  `rAND = min(rA, rB)`

  â†’ Combined score limited by the weaker (smaller) rank. Safe way to demand "both high."

* ğŸŒˆ **Liberal (OR-like) combination:**

  `rOR = max(rA, rB)`

  â†’ Combined score benefits from the stronger (larger) rank. Allows "either high."

These match **Day 1's logic mapping:**

ğŸ§© AND â‰ˆ min  OR â‰ˆ max

They're simple, monotone, and explainable âœ…

![Combining Ranks](/DS-4/combining_ranks.png)

---

## ğŸ§­ Stratification from Combined Ranks

Once you have a single combined rank per observation (e.g., rAND), split the population into **strata**:

* ğŸ”¢ **Deciles:** [0.0, 0.1, 0.2, â€¦, 0.9, 1.0]

* ğŸ§® **Quartiles:** [0.0, 0.25, 0.5, 0.75, 1.0]

* âš–ï¸ **Custom cuts:** e.g., [0.0, 0.2, 0.5, 0.8, 1.0]

Use strata to:

* ğŸ¯ Draw balanced samples

* ğŸš¦ Prioritize reviews/interventions

* ğŸ“Š Report performance metrics by difficulty bands

Because ranks are monotone-invariant, your strata stay meaningful even if raw features change scale.

![Stratification Concept](/DS-4/stratification_concept.png)

---

## ğŸ§© Solved example â€” From raw features to strata

<table>
<thead>
<tr>
<th >id</th>
<th >A</th>
<th >B</th>
</tr>
</thead>
<tbody>
<tr>
<td >1</td>
<td >10</td>
<td >5</td>
</tr>
<tr>
<td >2</td>
<td >7</td>
<td >3</td>
</tr>
<tr>
<td >3</td>
<td >12</td>
<td >9</td>
</tr>
<tr>
<td >4</td>
<td >15</td>
<td >4</td>
</tr>
<tr>
<td >5</td>
<td >8</td>
<td >8</td>
</tr>
<tr>
<td >6</td>
<td >20</td>
<td >6</td>
</tr>
<tr>
<td >7</td>
<td >9</td>
<td >2</td>
</tr>
<tr>
<td >8</td>
<td >18</td>
<td >10</td>
</tr>
</tbody>
</table>

### Step 1ï¸âƒ£: Compute percentile ranks per feature

Use the empirical CDF (rank = i/n). Example (rounded):

<table>
<thead>
<tr>
<th >A</th>
<th >rankA</th>
<th >B</th>
<th >rankB</th>
</tr>
</thead>
<tbody>
<tr>
<td >7</td>
<td >0.12</td>
<td >2</td>
<td >0.12</td>
</tr>
<tr>
<td >8</td>
<td >0.25</td>
<td >3</td>
<td >0.25</td>
</tr>
<tr>
<td >9</td>
<td >0.38</td>
<td >4</td>
<td >0.38</td>
</tr>
<tr>
<td >10</td>
<td >0.50</td>
<td >5</td>
<td >0.50</td>
</tr>
<tr>
<td >12</td>
<td >0.62</td>
<td >6</td>
<td >0.62</td>
</tr>
<tr>
<td >15</td>
<td >0.75</td>
<td >8</td>
<td >0.75</td>
</tr>
<tr>
<td >18</td>
<td >0.88</td>
<td >9</td>
<td >0.88</td>
</tr>
<tr>
<td >20</td>
<td >1.00</td>
<td >10</td>
<td >1.00</td>
</tr>
</tbody>
</table>

### Step 2ï¸âƒ£: Combine with min (AND-like) and max (OR-like)

<table>
<thead>
<tr>
<th >id</th>
<th >A</th>
<th >B</th>
<th >rA</th>
<th >rB</th>
<th >rAND = min(rA,rB)</th>
<th >rOR = max(rA,rB)</th>
</tr>
</thead>
<tbody>
<tr>
<td >1</td>
<td >10</td>
<td >5</td>
<td >0.50</td>
<td >0.50</td>
<td >0.50</td>
<td >0.50</td>
</tr>
<tr>
<td >2</td>
<td >7</td>
<td >3</td>
<td >0.12</td>
<td >0.25</td>
<td >0.12</td>
<td >0.25</td>
</tr>
<tr>
<td >3</td>
<td >12</td>
<td >9</td>
<td >0.62</td>
<td >0.88</td>
<td >0.62</td>
<td >0.88</td>
</tr>
<tr>
<td >4</td>
<td >15</td>
<td >4</td>
<td >0.75</td>
<td >0.38</td>
<td >0.38</td>
<td >0.75</td>
</tr>
<tr>
<td >5</td>
<td >8</td>
<td >8</td>
<td >0.25</td>
<td >0.75</td>
<td >0.25</td>
<td >0.75</td>
</tr>
<tr>
<td >6</td>
<td >20</td>
<td >6</td>
<td >1.00</td>
<td >0.62</td>
<td >0.62</td>
<td >1.00</td>
</tr>
<tr>
<td >7</td>
<td >9</td>
<td >2</td>
<td >0.38</td>
<td >0.12</td>
<td >0.12</td>
<td >0.38</td>
</tr>
<tr>
<td >8</td>
<td >18</td>
<td >10</td>
<td >0.88</td>
<td >1.00</td>
<td >0.88</td>
<td >1.00</td>
</tr>
</tbody>
</table>

### Step 3ï¸âƒ£: Create strata from rAND

Cuts at 0.2, 0.5, 0.8 â†’

* ğŸ©µ Stratum 1: rAND < 0.2 â†’ ids 2, 7

* ğŸ’™ Stratum 2: 0.2 â‰¤ rAND < 0.5 â†’ ids 5, 4

* ğŸ’œ Stratum 3: 0.5 â‰¤ rAND < 0.8 â†’ ids 1, 3, 6

* ğŸ’› Stratum 4: rAND â‰¥ 0.8 â†’ id 8

ğŸ“ˆ Using min is conservative: an observation only scores high if **both A and B** are high.

Using max is liberal â€” more points rise into higher strata.

![Solved Example Visualization](/DS-4/solved_example.png)

---

## ğŸ’­ Why "min of ranks" is conservative

* rAND = min(rA, rB) can never exceed either input.

* Raising any rank can only lift (not drop) rAND.

* Thus, demanding high rAND â‰ˆ saying "both inputs are high." âœ…

![Min vs Max Comparison](/DS-4/min_max_comparison.png)

---

## ğŸ–¼ï¸ Visual ideas

* ğŸ§Š Two 2-D heatmaps:

  1ï¸âƒ£ rA (x-axis = id, y-axis = rank) and rB similarly.

  2ï¸âƒ£ The combined min(rA,rB) mesh â€” shows the "AND valley."

* ğŸ“Š Simple bar chart: rAND per id colored by stratum.

![Visual Heatmaps](/DS-4/visual_heatmaps.png)

---

## ğŸ§  Practical tips

* ğŸ“¦ Compute ranks *per group* for fair comparison (region/time).

* ğŸ§¾ Handle ties consistently (average ranks).

* ğŸªœ Pre-decide strata cuts (deciles, quartiles, custom).

* ğŸ”„ Extend beyond two features:

  * rAND = min(râ‚,â€¦,râ‚–)

  * rOR  = max(râ‚,â€¦,râ‚–)

---

## ğŸŒŸ Takeaway

[Percentile ranks](/key) normalize features onto a **common [0,1] scale**.

Combining them with **min (AND)** or **max (OR)** gives an interpretable, monotone score ideal for **sampling, prioritization, and reporting.**

Simple âœ… Robust ğŸ§© Explainable ğŸ’¡

---

## ğŸ“š References

1. Hyndman, R. J., & Fan, Y. (1996). Sample quantiles in statistical packages. *The American Statistician*, 50(4), 361-365.

2. Serfling, R. J. (2009). *Approximation Theorems of Mathematical Statistics*. John Wiley & Sons.

3. Mosteller, F., & Tukey, J. W. (1977). *Data Analysis and Regression: A Second Course in Statistics*. Addison-Wesley.

4. Hoaglin, D. C., Mosteller, F., & Tukey, J. W. (Eds.). (1983). *Understanding Robust and Exploratory Data Analysis*. John Wiley & Sons.

5. David, H. A., & Nagaraja, H. N. (2003). *Order Statistics* (3rd ed.). John Wiley & Sons.

6. Parzen, E. (1979). Nonparametric statistical data modeling. *Journal of the American Statistical Association*, 74(365), 105-121.

7. Koenker, R. (2005). *Quantile Regression*. Cambridge University Press.

8. Langford, E. (2006). Quartiles in elementary statistics. *Journal of Statistics Education*, 14(3).

9. Hyndman, R. J. (1996). Computing and graphing highest density regions. *The American Statistician*, 50(2), 120-126.

10. CramÃ©r, H. (1946). *Mathematical Methods of Statistics*. Princeton University Press.

---

<div style={{textAlign: 'center', margin: '3rem 0', padding: '2rem', background: 'linear-gradient(135deg, #667eea 0%, #764ba2 100%)', borderRadius: '15px', color: 'white'}}>
  <div id="lottie-celebration" style={{width: '200px', height: '200px', margin: '0 auto'}}></div>
  <h3 style={{margin: '1rem 0', color: 'white'}}>Day 4 Complete! ğŸ‰</h3>
  <p style={{margin: 0, fontSize: '1.1rem', opacity: 0.9}}>*This is Day 4 of my 30-day challenge documenting my Data Science journey at Oracle! Stay tuned for more insights and mathematical foundations of data science. ğŸš€*</p>
  <div style={{marginTop: '1.5rem'}}>
    <span style={{background: 'rgba(255,255,255,0.2)', padding: '0.5rem 1rem', borderRadius: '25px', fontSize: '0.9rem'}}>Next: Day 5 - Coming Tomorrow!</span>
  </div>
</div>

